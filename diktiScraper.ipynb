{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas\n",
    "import MySQLdb\n",
    "import os\n",
    "\n",
    "\n",
    "db = MySQLdb.connect(\n",
    "    host = '127.0.0.1',\n",
    "    user = 'root',\n",
    "    passwd = '310116',\n",
    "    db = 'diktiScraperdb'\n",
    ")\n",
    "print \"Databases Connected\"\n",
    "class scrape(object): #membuat objek scrape\n",
    "\n",
    "    def __init__(self, nameOfMachine): #inisiasi objek\n",
    "        self.nameOfMachine = nameOfMachine #pemberian atribut objek pada inisiasi\n",
    "\n",
    "    def getLink(self, addr): #membuat fungsi getLink\n",
    "        dataLink = {  #membuat dictionary\n",
    "        'addt' : [], #teks untuk penambahan\n",
    "        'link' : [] #link yang ada di halaman\n",
    "        }\n",
    "        def scrapePage(addr):\n",
    "            scrapeData = BeautifulSoup(urllib.urlopen(addr), \"html.parser\")\n",
    "            return scrapeData\n",
    "        data = scrapePage(addr)\n",
    "        try :\n",
    "            for recordRow in data.findAll('tr'):\n",
    "                addtText = \"\"\n",
    "                for recordLink in recordRow.findAll('a', href=True):\n",
    "                    tempData = recordLink.text.split()\n",
    "                    for listOfTempData in tempData:\n",
    "                        addtText = addtText + listOfTempData + \" \"\n",
    "                    dataLink['addt'].append(str(addtText))\n",
    "                    dataLink['link'].append(str(recordLink['href']))\n",
    "            print \"Get DataLink Scrape HTML Object\"        \n",
    "            return dataLink\n",
    "        except AttributeError :\n",
    "            print \"Not Found\"\n",
    "            return dataLink\n",
    "    def getTextOnly(self,addr):\n",
    "        dataText = {\n",
    "            'addt' : []\n",
    "        }\n",
    "        listText = []\n",
    "        def scrapePage(addr):\n",
    "            scrapeData = BeautifulSoup(urllib.urlopen(addr), \"html.parser\")\n",
    "            return scrapeData\n",
    "        data = scrapePage(addr)\n",
    "        try:\n",
    "            for recordRow in data.findAll('tr'):\n",
    "                for recordText in recordRow.findAll('td'):\n",
    "                    listText.append(recordText.text)\n",
    "            for item in listText[2::21]:\n",
    "                addtText = \"\"\n",
    "                tempData = item.split()\n",
    "                for listOfTempData in tempData:\n",
    "                    addtText = addtText + listOfTempData + \" \"\n",
    "                dataText['addt'].append(str(addtText))\n",
    "                #print addtText\n",
    "            print \"Get Text Faculty ScrapeHTML Object\"\n",
    "            return dataText\n",
    "        except AttributeError:\n",
    "            print \"Not Found\"\n",
    "            return dataText\n",
    "\n",
    "urlServer = \"/home/alien/diktiScraper\" #server program host\n",
    "urlTarget = \"http://forlap.dikti.go.id/perguruantinggi/homerekap\" #home rekap\n",
    "dataGet = {\n",
    "    'addt' : [],\n",
    "    'link' : []\n",
    "} #dictionary for data that got from scraper\n",
    "scraperLayer0 = scrape('layer0_df')\n",
    "dataGet = scraperLayer0.getLink(urlTarget)\n",
    "layer0_df = pandas.DataFrame(dataGet, columns=['addt', 'link'])\n",
    "layer0_df.index.name = 'institutionId'\n",
    "#print layer0_df\n",
    "cur = db.cursor()\n",
    "cur.execute('USE diktiScraperdb;') #menggunakan db\n",
    "cur.execute('DROP TABLE IF EXISTS institusi;') #mengecek tablenya sudah ada atau belum\n",
    "cur.execute('CREATE TABLE institusi (institutionId INT NOT NULL PRIMARY KEY, addt VARCHAR(255),link VARCHAR(255));') #buat table\n",
    "layer0_df.to_sql(con=db,\n",
    "                 name='institusi', \n",
    "                 if_exists='append', \n",
    "                 flavor='mysql',              \n",
    "                ) #mencetak ke mySQL\n",
    "print \"Data Layer 0 transfered to SQL, done\"\n",
    "layer0_df = pandas.read_sql(\"SELECT link FROM institusi;\" ,db)\n",
    "print \"Data load from SQL layer0_df\"\n",
    "indexUniFac = \"indexUniFac\"\n",
    "query7 = \"DROP TABLE IF EXISTS  \"+indexUniFac+\";\"\n",
    "query8 = \"CREATE TABLE \"+indexUniFac+\" (Id INT NOT NULL PRIMARY KEY,code VARCHAR(255), univ VARCHAR(255), fac VARCHAR(255));\"\n",
    "cur.execute(query7)\n",
    "cur.execute(query8)\n",
    "indexUniFac = {\n",
    "    'code' : [],\n",
    "    'univ' : [],\n",
    "    'fac' : []\n",
    "}\n",
    "l = 0\n",
    "i = 0\n",
    "#layer = 1\n",
    "for link in layer0_df['link']:\n",
    "    urlTarget0 = link\n",
    "    #print urlTarget\n",
    "    if i/10 < 1:\n",
    "        nameDb1 = 'list0' + str(i)\n",
    "    else :\n",
    "        nameDb1 = 'list' + str(i)\n",
    "    nameDb1 = str(nameDb1)\n",
    "    query0 = 'USE diktiScraperdb;'\n",
    "    query1 = \"DROP TABLE IF EXISTS \" + nameDb1\n",
    "    query2 = \"CREATE TABLE \"+ nameDb1 +\" (institutionId INT NOT NULL PRIMARY KEY, addt VARCHAR(255),link VARCHAR(255));\"\n",
    "    scraper1 = scrape('df1')\n",
    "    dataGet1 = scraper1.getLink(urlTarget0)\n",
    "    df1 = pandas.DataFrame(dataGet1, columns=['addt', 'link'])\n",
    "    df1.index.name = 'institutionId'\n",
    "    #print df\n",
    "    cur.execute(query0)\n",
    "    cur.execute(query1)\n",
    "    cur.execute(query2)\n",
    "    df1.to_sql(con=db,\n",
    "            name= nameDb1, \n",
    "            if_exists='append', \n",
    "            flavor='mysql',\n",
    "            ) #mencetak ke mySQL\n",
    "    print nameDb1 +\" transfered to SQL, done\"\n",
    "    query3 = \"SELECT link FROM \"+ nameDb1 +\";\"\n",
    "    layer1_df = pandas.read_sql(query3 ,db)\n",
    "    print \"Data load from SQL \" + nameDb1\n",
    "    j = 0\n",
    "    for link1 in  layer1_df['link']:\n",
    "        urlTarget1 = link1\n",
    "        if j/10 < 1:\n",
    "            nameDb2 = nameDb1 + \"0\" + str(j)\n",
    "        else :\n",
    "            nameDb2 = nameDb1 + str(j)\n",
    "        nameDb2 = str(nameDb2)\n",
    "        query4 = \"DROP TABLE IF EXISTS \" + nameDb2\n",
    "        query5 = \"CREATE TABLE \"+ nameDb2 +\" (institutionId INT NOT NULL PRIMARY KEY, addt VARCHAR(255),link VARCHAR(255));\"\n",
    "        scraper2 = scrape('df2')\n",
    "        dataGet2 = scraper2.getLink(urlTarget1)\n",
    "        df2 = pandas.DataFrame(dataGet2, columns=['addt', 'link'])\n",
    "        df2.index.name = 'institutionId'\n",
    "        cur.execute(query0)\n",
    "        cur.execute(query4)\n",
    "        cur.execute(query5)\n",
    "        df2.to_sql(con=db,\n",
    "                  name = nameDb2,\n",
    "                  if_exists=\"append\",\n",
    "                  flavor=\"mysql\",\n",
    "                  )\n",
    "        print nameDb2 +\" transfered to SQL, done\"\n",
    "        query6 = \"SELECT * FROM \"+ nameDb2 +\";\"\n",
    "        layer2_df = pandas.read_sql(query6 ,db)\n",
    "        print \"Data load from SQL \" + nameDb2\n",
    "        k = 0\n",
    "        for link2 in  layer2_df['link']:\n",
    "            urlTarget2 = link2\n",
    "            if k/10 < 1:\n",
    "                nameDb3 = nameDb2 + \"0\" + str(k)\n",
    "            else :\n",
    "                nameDb3 = nameDb2 + str(k)\n",
    "            nameDb3 = str(nameDb3)\n",
    "            query9 = \"DROP TABLE IF EXISTS \" + nameDb3 +\";\"\n",
    "            query10 = \"CREATE TABLE \"+ nameDb3 +\" (institutionId INT NOT NULL PRIMARY KEY, addt VARCHAR(255));\"\n",
    "            scraper3 = scrape('df3')\n",
    "            dataGet3 = scraper3.getTextOnly(urlTarget2)\n",
    "            df3 = pandas.DataFrame(dataGet3, columns=['addt'])\n",
    "            df3.index.name = 'institutionId'\n",
    "            m = 0\n",
    "            for falcu in df3['addt']:\n",
    "                if m/10 < 1:\n",
    "                    code = nameDb3[4:] + \"00000\" + str(m)\n",
    "                elif m/100 <1:\n",
    "                    code = nameDb3[4:] + \"0000\" + str(m)\n",
    "                elif m/1000 <1:\n",
    "                    code = nameDb3[4:] + \"000\" + str(m)\n",
    "                elif l/10000 <1:\n",
    "                    code = nameDb3[4:] + \"00\" + str(m)\n",
    "                elif m/100000 <1:\n",
    "                    code = nameDb3[4:] + \"0\" + str(m)\n",
    "                else :\n",
    "                    code = nameDb3[4:] + str(m)\n",
    "                indexUniFac['code'].append(code)\n",
    "                indexUniFac['univ'].append(layer2_df['addt'][k])\n",
    "                indexUniFac['fac'].append(falcu)\n",
    "                print \"adding Dict indexUniFac \"+ indexUniFac['code'][l] + \" | \" + indexUniFac['univ'][l] + \" | \" + indexUniFac['fac'][l]\n",
    "                m= m+1\n",
    "                l = l+1\n",
    "            cur.execute(query0)\n",
    "            cur.execute(query9)\n",
    "            cur.execute(query10)\n",
    "            #print df3\n",
    "            df3.to_sql(con=db,\n",
    "                      name = nameDb3,\n",
    "                      if_exists=\"append\",\n",
    "                      flavor=\"mysql\",\n",
    "                      )\n",
    "            print nameDb3 +\" transfered to SQL, done\"\n",
    "            k = k+1\n",
    "        j = j+1\n",
    "    i = i+1\n",
    "df4 = pandas.DataFrame(indexUniFac, columns=['code', 'univ', 'fac'])\n",
    "df4.index.name = \"id\"\n",
    "df4.to_sql(con = db,\n",
    "          name = indexUniFac,\n",
    "          if_exists=\"append\",\n",
    "          flavor=\"mysql\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
