{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get DataLink Scrape HTML Object\n",
      "Get DataLink Scrape HTML Object\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get DataLink Scrape HTML Object\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': [], 'addt': []}\n",
      "Get Text ScrapeHTML Object\n",
      "{'code': ['001001 ', '001002 ', '001003 ', '001004 ', '001005 ', '001006 ', '001007 ', '001008 ', '001009 ', '001010 ', '001011 ', '001012 ', '001013 ', '001014 ', '001015 ', '001016 ', '001017 ', '001018 ', '001019 ', '001020 ', '001021 ', '001022 ', '001023 ', '001024 ', '001025 ', '001026 ', '001027 ', '001028 ', '001029 ', '001030 ', '001031 ', '001032 ', '001033 ', '001034 ', '001035 ', '001036 ', '001037 ', '001038 ', '001039 ', '001040 ', '001041 ', '001042 ', '001043 ', '001044 ', '001045 ', '001046 ', '001047 ', '001048 ', '001049 ', '001050 ', '001051 ', '001052 ', '001053 ', '001054 ', '001055 ', '001056 ', '001057 ', '001058 ', '001059 ', '001060 ', '001061 ', '001062 ', '001063 ', '002001 ', '002002 ', '002003 ', '002005 ', '002007 ', '002008 ', '002009 ', '002010 ', '002011 ', '002012 ', '002013 ', '002014 ', '002015 ', '005001 ', '005002 ', '005003 ', '005004 ', '005005 ', '005006 ', '005007 ', '005008 ', '005009 ', '005010 ', '005011 ', '005012 ', '005013 ', '005014 ', '005015 ', '005016 ', '005017 ', '005018 ', '005019 ', '005020 ', '005021 ', '005022 ', '005023 ', '005024 ', '005025 ', '005026 ', '005027 ', '005028 ', '005029 ', '005030 ', '005031 ', '005032 ', '005033 ', '005034 ', '005035 ', '005036 ', '005037 ', '005038 ', '005039 ', '005040 ', '005041 ', '005042 ', '005043 ', '006001 ', '006002 ', '006003 '], 'addt': ['Universitas Gadjah Mada ', '99.51 ', '100 ', '100 ', '93.23 ', '100 ', '96.64 ', '88.89 ', '- ', '001011 ', '98.46 ', '100 ', '97.87 ', '100 ', '92.16 ', '100 ', '- ', '0.64 ', '21 ', 'Universitas Tanjungpura ', '100 ', '100 ', '95.08 ', '97.14 ', '99.3 ', '95.31 ', '13.95 ', '- ', '001032 ', '100 ', '100 ', '97.87 ', '97.62 ', '94.57 ', '92.31 ', '100 ', '- ', '42 ', 'Universitas Trunojoyo ', '100 ', '100 ', '100 ', '92.31 ', '100 ', '92.86 ', '94.44 ', '- ', '001053 ', '- ', '- ', '- ', '- ', '- ', '100 ', '100 ', '- ', '63 ', 'Institut Teknologi Bandung ', '98.15 ', '96.2 ', '100 ', '100 ', '100 ', '100 ', '11.11 ', '- ', '002013 ', '- ', '- ', '100 ', '100 ', '100 ', '100 ', '100 ', '- ', '84 ', 'Politeknik Negeri Padang ', '100 ', '100 ', '100 ', '94.12 ', '100 ', '100 ', '94.74 ', '- ', '005019 ', '100 ', '100 ', '100 ', '100 ', '100 ', '100 ', '70.37 ', '- ', '105 ', 'Politeknik Negeri Nusa Utara ', '- ', '- ', '- ', '100 ', '100 ', '100 ', '100 ', '- ', '005040 ', '- ', '- ', '- ', '- ', '- ', '- ']}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7f79448033f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mscraper2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mdataGet2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscraper2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTextOnly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlTarget1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataGet2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'addt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'institutionId'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alien/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    222\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    223\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alien/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;31m# raise ValueError if only scalars in dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                 \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;31m# prefilter if columns passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alien/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   5277\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5278\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5279\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'arrays must all be same length'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from socket import error as SocketError\n",
    "import urllib\n",
    "import pandas\n",
    "import MySQLdb\n",
    "import re\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "db = MySQLdb.connect(\n",
    "    host = '127.0.0.1',\n",
    "    user = 'root',\n",
    "    passwd = '310116',\n",
    "    db = 'diktiScraperdb'\n",
    ")\n",
    "class scrape(object): #membuat objek scrape\n",
    "\n",
    "    def __init__(self, nameOfMachine): #inisiasi objek\n",
    "        self.nameOfMachine = nameOfMachine #pemberian atribut objek pada inisiasi\n",
    "\n",
    "    def getLink(self, addr): #membuat fungsi getLink\n",
    "        dataLink = {  #membuat dictionary\n",
    "        'addt' : [], #teks untuk penambahan\n",
    "        'link' : [] #link yang ada di halaman\n",
    "        }\n",
    "        def scrapePage(addr):\n",
    "            scrapeData = BeautifulSoup(urllib.urlopen(addr), \"html.parser\")\n",
    "            return scrapeData\n",
    "        data = scrapePage(addr)\n",
    "        try :\n",
    "            for recordRow in data.findAll('tr'):\n",
    "                addtText = \"\"\n",
    "                for recordLink in recordRow.findAll('a', href=True):\n",
    "                    tempData = recordLink.text.split()\n",
    "                    for listOfTempData in tempData:\n",
    "                        addtText = addtText + listOfTempData + \" \"\n",
    "                    dataLink['addt'].append(str(addtText))\n",
    "                    dataLink['link'].append(str(recordLink['href']))\n",
    "            print \"Get DataLink Scrape HTML Object\"        \n",
    "            return dataLink\n",
    "        except AttributeError :\n",
    "            print \"Not Found\"\n",
    "            return dataLink\n",
    "    def getTextOnly(self,addr):\n",
    "        dataText = {\n",
    "            'code' : [],\n",
    "            'univ' : []\n",
    "        }\n",
    "        listCode = []\n",
    "        listUniv = []\n",
    "        def scrapePage(addr):\n",
    "            scrapeData = BeautifulSoup(urllib.urlopen(addr), \"html.parser\")\n",
    "            return scrapeData\n",
    "        data = scrapePage(addr)\n",
    "        try:\n",
    "            for recordRow in data.findAll('tr'):\n",
    "                for recordText in recordRow.findAll('td'):\n",
    "                    listCode.append(recordText.text)\n",
    "                    listUniv.append(recordText.text)\n",
    "            for item in listCode[1::19]:\n",
    "                addtText = \"\"\n",
    "                tempData = item.split()\n",
    "                for listOfTempData in tempData:\n",
    "                    addtText = addtText + listOfTempData + \" \"\n",
    "                dataText['code'].append(str(addtText))\n",
    "                #print addtText\n",
    "            for item in listUniv[2::19]:\n",
    "                addtText = \"\"\n",
    "                tempData = item.split()\n",
    "                for listOfTempData in tempData:\n",
    "                    addtText = addtText + listOfTempData + \" \"\n",
    "                dataText['univ'].append(str(addtText))\n",
    "                #print addtText \n",
    "            print \"Get Text ScrapeHTML Object\"\n",
    "            #print dataText\n",
    "            return dataText\n",
    "        except SocketError as e:\n",
    "            if e.errno != errno.ECONNRESET:\n",
    "                raise # Not error we are looking for\n",
    "            pass # Handle error here.\n",
    "\n",
    "urlServer = \"/home/alien/diktiScraper\" #server program host\n",
    "urlTarget = \"http://forlap.dikti.go.id/perguruantinggi/homerekap\" #home rekap\n",
    "scraperLayer0 = scrape('layer0_df')\n",
    "dataGet = scraperLayer0.getLink(urlTarget)\n",
    "layer0_df = pandas.DataFrame(dataGet, columns=['addt', 'link'])\n",
    "layer0_df.index.name = 'institutionId'\n",
    "cur = db.cursor()\n",
    "query0 = 'USE diktiScraperdb;'\n",
    "cur.execute(query0)\n",
    "indexUniv = \"indexUniv\"\n",
    "query7 = \"DROP TABLE IF EXISTS  \"+indexUniv+\";\"\n",
    "query8 = \"CREATE TABLE \"+indexUniv+\" (Id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,code VARCHAR(255),inst VARCHAR(255), univ VARCHAR(255));\"\n",
    "cur.execute(query7)\n",
    "cur.execute(query8)\n",
    "l = 0\n",
    "i = 0\n",
    "for link in layer0_df['link']:\n",
    "    urlTarget0 = link\n",
    "    print str(i) + \" \" + layer0_df['addt'][i]\n",
    "    if i == 0 or i > 2 :\n",
    "        scraper1 = scrape('df1')\n",
    "        dataGet1 = scraper1.getTextOnly(urlTarget0)\n",
    "        df1 = pandas.DataFrame(dataGet1, columns=['code', 'univ'])\n",
    "        df1.index.name = 'institutionId'\n",
    "        j = 0\n",
    "        for univ in df1['univ']:\n",
    "            a = df1['code'][j]\n",
    "            a = re.sub(\"[!@#$/']\", '', a)\n",
    "            b = layer0_df['addt'][i] #inst\n",
    "            b = re.sub(\"[!@#$/']\", '', b)\n",
    "            c = univ #univ\n",
    "            c = re.sub(\"[!@#$/']\", '', c)\n",
    "            print \" > \" + a + \" | \" + b + \" | \" + c\n",
    "            query11 = \"INSERT INTO indexUniv (code, inst, univ) VALUES ('\"+a+\"','\"+b+\"','\"+c+\"');\"\n",
    "            cur.execute(query11)\n",
    "            db.commit()\n",
    "            j = j + 1\n",
    "    elif i == 2:\n",
    "        #\n",
    "        scraper1 = scrape('df1')\n",
    "        dataGet1 = scraper1.getLink(urlTarget0)\n",
    "        df1 = pandas.DataFrame(dataGet1, columns=['addt', 'link'])\n",
    "        df1.index.name = 'institutionId'\n",
    "        j = 0\n",
    "        for link1 in  df1['link']:\n",
    "            urlTarget1 = link1\n",
    "            scraper2 = scrape('df2')\n",
    "            dataGet2 = scraper2.getLink(urlTarget1)\n",
    "            df2 = pandas.DataFrame(dataGet2, columns=['addt', 'link'])\n",
    "            df2.index.name = 'institutionId'\n",
    "            k = 0\n",
    "            for link2 in df2['link']:\n",
    "                urlTarget2 = link2\n",
    "                scraper3 = scrape('df3')\n",
    "                dataGet3 = scraper3.getTextOnly(urlTarget2)\n",
    "                df3 = pandas.DataFrame(dataGet3, columns=['code', 'univ'])\n",
    "                df3.index.name = 'institutionId'\n",
    "                l = 0\n",
    "                for univ in df3['univ']:\n",
    "                    a = df3['code'][l] #code\n",
    "                    a = re.sub(\"[!@#$/']\", '', a)\n",
    "                    b = layer0_df['addt'][i] #inst\n",
    "                    b = re.sub(\"[!@#$/']\", '', b)\n",
    "                    c = df1['addt'][j] #inst\n",
    "                    c = re.sub(\"[!@#$/']\", '', c)\n",
    "                    d = df2['addt'][k] #inst\n",
    "                    d = re.sub(\"[!@#$/']\", '', d)\n",
    "                    d = b + \" \" + c + \" \" + d\n",
    "                    e = univ\n",
    "                    e = re.sub(\"[!@#$/']\", '', e)\n",
    "                    print \" > \" + a + \" | \" + d + \" | \" + e\n",
    "                    query11 = \"INSERT INTO indexUniv (code, inst, univ) VALUES ('\"+a+\"','\"+d+\"','\"+e+\"');\"\n",
    "                    cur.execute(query11)\n",
    "                    db.commit()\n",
    "                    l = l+1\n",
    "                k = k+1\n",
    "            j = j+1\n",
    "    elif i == 1 :\n",
    "        scraper1 = scrape('df1')\n",
    "        dataGet1 = scraper1.getLink(urlTarget0)\n",
    "        df1 = pandas.DataFrame(dataGet1, columns=['addt', 'link'])\n",
    "        df1.index.name = 'institutionId'\n",
    "        j = 0\n",
    "        for link1 in  df1['link']:\n",
    "            urlTarget1 = link1\n",
    "            scraper2 = scrape('df2')\n",
    "            dataGet2 = scraper2.getTextOnly(urlTarget1)\n",
    "            df2 = pandas.DataFrame(dataGet2, columns=['code', 'univ'])\n",
    "            df2.index.name = 'institutionId'\n",
    "            k = 0\n",
    "            for univ in df2['univ']:\n",
    "                a = df2['code'][k] #code\n",
    "                a = re.sub(\"[!@#$/']\", '', a)\n",
    "                b = layer0_df['addt'][i] #inst\n",
    "                b = re.sub(\"[!@#$/']\", '', b)\n",
    "                c = df1['addt'][j] #inst\n",
    "                c = re.sub(\"[!@#$/']\", '', c)\n",
    "                c = b + \" \" + c\n",
    "                d = univ #univ\n",
    "                d = re.sub(\"[!@#$/']\", '', d)\n",
    "                print \" > \" + a + \" | \" + c + \" | \" + d\n",
    "                query11 = \"INSERT INTO indexUniv (code, inst, univ) VALUES ('\"+a+\"','\"+c+\"','\"+d+\"');\"\n",
    "                cur.execute(query11)\n",
    "                db.commit()\n",
    "                k = k+1\n",
    "            j = j+1\n",
    "    i = i+1\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
