{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from socket import error as SocketError\n",
    "import urllib\n",
    "import pandas\n",
    "import MySQLdb\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "ts = time.time()\n",
    "st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "db = MySQLdb.connect(\n",
    "    host = '127.0.0.1',\n",
    "    user = 'root',\n",
    "    passwd = '310116',\n",
    "    db = 'diktiScraperdb'\n",
    ")\n",
    "class scrape(object): #membuat objek scrape\n",
    "\n",
    "    def __init__(self, nameOfMachine): #inisiasi objek\n",
    "        self.nameOfMachine = nameOfMachine #pemberian atribut objek pada inisiasi\n",
    "\n",
    "    def getLink(self, addr): #membuat fungsi getLink\n",
    "        dataLink = {  #membuat dictionary\n",
    "        'addt' : [], #teks untuk penambahan\n",
    "        'link' : [] #link yang ada di halaman\n",
    "        }\n",
    "        def scrapePage(addr):\n",
    "            scrapeData = BeautifulSoup(urllib.urlopen(addr), \"html.parser\")\n",
    "            return scrapeData\n",
    "        data = scrapePage(addr)\n",
    "        try :\n",
    "            for recordRow in data.findAll('tr'):\n",
    "                addtText = \"\"\n",
    "                for recordLink in recordRow.findAll('a', href=True):\n",
    "                    tempData = recordLink.text.split()\n",
    "                    for listOfTempData in tempData:\n",
    "                        addtText = addtText + listOfTempData + \" \"\n",
    "                    dataLink['addt'].append(str(addtText))\n",
    "                    dataLink['link'].append(str(recordLink['href']))\n",
    "            print \"Get DataLink Scrape HTML Object\"        \n",
    "            return dataLink\n",
    "        except AttributeError :\n",
    "            print \"Not Found\"\n",
    "            return dataLink\n",
    "    def getTextOnly(self,addr):\n",
    "        dataText = {\n",
    "            'addt' : []\n",
    "        }\n",
    "        listText = []\n",
    "        def scrapePage(addr):\n",
    "            scrapeData = BeautifulSoup(urllib.urlopen(addr), \"html.parser\")\n",
    "            return scrapeData\n",
    "        data = scrapePage(addr)\n",
    "        try:\n",
    "            for recordRow in data.findAll('tr'):\n",
    "                for recordText in recordRow.findAll('td'):\n",
    "                    listText.append(recordText.text)\n",
    "            for item in listText[2::21]:\n",
    "                addtText = \"\"\n",
    "                tempData = item.split()\n",
    "                for listOfTempData in tempData:\n",
    "                    addtText = addtText + listOfTempData + \" \"\n",
    "                dataText['addt'].append(str(addtText))\n",
    "                #print addtText\n",
    "            print \"Get Text Faculty ScrapeHTML Object\"\n",
    "            return dataText\n",
    "        except SocketError as e:\n",
    "            if e.errno != errno.ECONNRESET:\n",
    "                raise # Not error we are looking for\n",
    "            pass # Handle error here.\n",
    "\n",
    "urlServer = \"/home/alien/diktiScraper\" #server program host\n",
    "urlTarget = \"http://forlap.dikti.go.id/perguruantinggi/homerekap\" #home rekap\n",
    "dataGet = {\n",
    "    'addt' : [],\n",
    "    'link' : []\n",
    "} #dictionary for data that got from scraper\n",
    "scraperLayer0 = scrape('layer0_df')\n",
    "dataGet = scraperLayer0.getLink(urlTarget)\n",
    "layer0_df = pandas.DataFrame(dataGet, columns=['addt', 'link'])\n",
    "cur = db.cursor()\n",
    "query0 = 'USE diktiScraperdb;'\n",
    "cur.execute(query0)\n",
    "indexUniFac = \"indexUniFac\"\n",
    "query7 = \"DROP TABLE IF EXISTS  \"+indexUniFac+\";\"\n",
    "query8 = \"CREATE TABLE \"+indexUniFac+\" (Id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,code VARCHAR(255), univ VARCHAR(255), fac VARCHAR(255));\"\n",
    "cur.execute(query7)\n",
    "cur.execute(query8)\n",
    "indexUniFac = {\n",
    "    'code' : [],\n",
    "    'univ' : [],\n",
    "    'fac' : []\n",
    "}\n",
    "l = 0\n",
    "i = 0\n",
    "for link in layer0_df['link']:\n",
    "    urlTarget0 = link\n",
    "    if i/10 < 1:\n",
    "        nameDb1 = 'list0' + str(i)\n",
    "    else :\n",
    "        nameDb1 = 'list' + str(i)\n",
    "    nameDb1 = str(nameDb1)\n",
    "    scraper1 = scrape('df1')\n",
    "    dataGet1 = scraper1.getLink(urlTarget0)\n",
    "    df1 = pandas.DataFrame(dataGet1, columns=['addt', 'link'])\n",
    "    df1.index.name = 'institutionId'\n",
    "    j = 0\n",
    "    for link1 in  df1['link']:\n",
    "        urlTarget1 = link1\n",
    "        if j/10 < 1:\n",
    "            nameDb2 = nameDb1 + \"0\" + str(j)\n",
    "        else :\n",
    "            nameDb2 = nameDb1 + str(j)\n",
    "        nameDb2 = str(nameDb2)\n",
    "        scraper2 = scrape('df2')\n",
    "        dataGet2 = scraper2.getLink(urlTarget1)\n",
    "        df2 = pandas.DataFrame(dataGet2, columns=['addt', 'link'])\n",
    "        df2.index.name = 'institutionId'\n",
    "        k = 0\n",
    "        for link2 in  df2['link']:\n",
    "            urlTarget2 = link2\n",
    "            if k/10 < 1:\n",
    "                nameDb3 = nameDb2 + \"0\" + str(k)\n",
    "            else :\n",
    "                nameDb3 = nameDb2 + str(k)\n",
    "            nameDb3 = str(nameDb3)\n",
    "            scraper3 = scrape('df3')\n",
    "            dataGet3 = scraper3.getTextOnly(urlTarget2)\n",
    "            df3 = pandas.DataFrame(dataGet3, columns=['addt'])\n",
    "            df3.index.name = 'institutionId'\n",
    "            m = 0\n",
    "            print st\n",
    "            for falcu in df3['addt']:\n",
    "                if m/10 < 1:\n",
    "                    code = nameDb3[4:] + \"00000\" + str(m)\n",
    "                elif m/100 <1:\n",
    "                    code = nameDb3[4:] + \"0000\" + str(m)\n",
    "                elif m/1000 <1:\n",
    "                    code = nameDb3[4:] + \"000\" + str(m)\n",
    "                elif l/10000 <1:\n",
    "                    code = nameDb3[4:] + \"00\" + str(m)\n",
    "                elif m/100000 <1:\n",
    "                    code = nameDb3[4:] + \"0\" + str(m)\n",
    "                else :\n",
    "                    code = nameDb3[4:] + str(m)\n",
    "                indexUniFac['code'].append(code)\n",
    "                indexUniFac['univ'].append(df2['addt'][k])\n",
    "                indexUniFac['fac'].append(falcu)\n",
    "                a = indexUniFac['code'][l]\n",
    "                a = re.sub(\"[!@#$/']\", '', a)\n",
    "                b = indexUniFac['univ'][l]\n",
    "                b = re.sub(\"[!@#$/']\", '', b)\n",
    "                c = indexUniFac['fac'][l]\n",
    "                c = re.sub(\"[!@#$/']\", '', c)\n",
    "                print \"adding Dict indexUniFac \"+ a + \" | \" + b + \" | \" + c\n",
    "                query11 = \"INSERT INTO indexUniFac (code, univ, fac) VALUES ('\"+a+\"','\"+b+\"','\"+c+\"');\" \n",
    "                cur.execute(query11)\n",
    "                db.commit()\n",
    "                print \"insert to DB MySQL\"\n",
    "                m= m+1\n",
    "                l = l+1\n",
    "            k = k+1\n",
    "        j = j+1\n",
    "    i = i+1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
